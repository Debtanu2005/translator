# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xo_USDJuuC7MbuH711CHYe7SN2S-rRhc
"""

# !pip install numpy
# !pip install pandas
# !pip install matplotlib
# !pip install scikit-learn
# !pip install tensorflow
# !pip install seaborn
# !pip install requests

import pandas as pd

df = pd.read_csv("eng_french.csv")

df

def count_words(text):
    if isinstance(text, str):
        words = text.split()
        return len(words)
    else:
        return 0

df['en_len']= df['English words/sentences'].apply(count_words)
df['fr_len']= df['French words/sentences'].apply(count_words)

print(max(df['en_len']))
print(max(df['fr_len']))



import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
axes[0].plot(df['en_len'])
axes[0].set_xlabel('X-axis')
axes[0].tick_params(axis='x', labelrotation=30)
axes[0].set_ylabel('Y-axis')
axes[1].plot(df['fr_len'])
axes[1].set_xlabel('X-axis')
axes[1].set_ylabel('Y-axis')
axes[1].tick_params(axis='x', labelrotation=30)

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Bidirectional, Dropout, Input
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical

tok = Tokenizer(oov_token="<OOV>")
tok.fit_on_texts(df['English words/sentences'])
seq = tok.texts_to_sequences(df['English words/sentences'])

max_input_len = max(len(sen) for sen in seq)
print(max_input_len)

print(tok.word_index)
total_input_words= len(tok.word_index)+1

print(type(tok.word_index))
print(len(tok.word_index))

tok_out = Tokenizer(oov_token="<OOV>")
tok_out.fit_on_texts(df['French words/sentences'])
seq_out = tok.texts_to_sequences(df['French words/sentences'])

print(tok_out.word_index)
total_input_words= len(tok_out.word_index)+1

print(len(tok_out.word_index))
max_output_words= len(tok_out.word_index)+1

encoder_input_seq= pad_sequences(seq, padding='post', maxlen=44)
decoder_input_seq= pad_sequences(seq_out, padding='post', maxlen=55)

print(encoder_input_seq)

encoder_input_seq.shape

decoder_input_seq.shape

final = [encoder_input_seq, decoder_input_seq]

embedding_layer = Embedding(total_input_words, 100, input_length=max_input_len)

import numpy as np

decoder_targets_one_hot = np.zeros((
        df['English words/sentences'].shape[0],
        55,
        max_output_words
    ),
    dtype='float32'
)
decoder_targets_one_hot.shape

for i in decoder_input_seq:
    for t, word in enumerate(i):
        decoder_targets_one_hot[i, t, word] = 1

num_ones = np.count_nonzero(decoder_targets_one_hot[172000] == 1)

decoder_input_seq[172000]

df['French words/sentences'][172000]

LSTM_nodes = 128

encoder_input = Input(shape=(44,))
x = embedding_layer(encoder_input)
encoder= LSTM(LSTM_nodes, return_state = True)
encoder_outputs, h, c = encoder(x)
encoder_state = [h,c]

decoder_input = Input(shape=(55,))
decoder_embedding = Embedding(max_output_words, 100)
decoder_embedding_output = decoder_embedding(decoder_input)
decoder = LSTM(LSTM_nodes, return_sequences=True, return_state=True)
decoder_outputs, h_1, c_1 = decoder(decoder_embedding_output, initial_state=encoder_state)
decoder_state= [h_1,c_1]

output_dense = Dense(max_output_words, activation='softmax')
output = output_dense(decoder_outputs)

model = Model ([encoder_input, decoder_input], output)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

from tensorflow.keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)

history = model.fit([encoder_input_seq, decoder_input_seq], decoder_targets_one_hot,
                    epochs= 5,
                    batch_size= 500,
                    callbacks= es
                    )

